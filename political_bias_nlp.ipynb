{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Next Word Prediction-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaperex/My_Projects/blob/master/political_bias_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNUFrq7lqHSg"
      },
      "source": [
        "### Importing The Required Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f151CllrqHSj",
        "outputId": "55ab63af-6abc-4230-da1f-8a55b14a4acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94f4WitEtTbB"
      },
      "source": [
        "# Mount Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSstXrpJqQ0r",
        "outputId": "185c6f71-fdae-41a9-f0fc-d6698f44d052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1yEexmEcDkn"
      },
      "source": [
        "def preprocess_sentence(base_path = '/content/drive/My Drive/NewB-master', file_name='example.txt',news_id='0'):\n",
        "    \"\"\"\n",
        "    news_id  1 - New York Times {Liberal}, 7 - New York Post {Conservative}\n",
        "    \"\"\"\n",
        "\n",
        "    with open(os.path.join(base_path,file_name), 'r') as f:\n",
        "        sentences = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            label, sentence = line.split('\\t')[0:2] # remove label and \\n\n",
        "            if label == news_id: \n",
        "                if len(sentence.split()) <=30:\n",
        "                    sentence_without_sw = [word for word in sentence if not word in stopwords.words()]\n",
        "\n",
        "                    sentences.append((\" \").join(sentence_without_sw))\n",
        "        print('num sentences:',len(sentences))\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer = Tokenizer(oov_token='oov_word')\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "    # saving the tokenizer for predict function.\n",
        "    pickle.dump(tokenizer, open(f'tokenizer_{file_name.split(\".\")[0]}.pkl', 'wb'))\n",
        "\n",
        "    sequence_data = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('vocab_size: ',vocab_size)\n",
        "\n",
        "    # compute targets\n",
        "    targets = [sentence[1:]+[0] for sentence in sequence_data]\n",
        "\n",
        "    # Add padding\n",
        "    padded_targets = pad_sequences(targets, padding=\"post\")\n",
        "    padded_inputs = pad_sequences(sequence_data, padding=\"post\")\n",
        "\n",
        "    return  padded_inputs, padded_targets, vocab_size"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkrsxL5FqHUr"
      },
      "source": [
        "### Creating the Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRsWCkwbqHUt"
      },
      "source": [
        "class nwp_model():\n",
        "    def __init__(self,vocab_size,model=None, embedding_dim=20, ):\n",
        "        if model:\n",
        "            self.model = model\n",
        "        else:\n",
        "            model = Sequential()\n",
        "            model.add(Embedding(vocab_size, output_dim=embedding_dim, mask_zero=True, input_length=30))\n",
        "            model.add(LSTM(40, return_sequences=True))\n",
        "            model.add(Dense(40, activation=\"relu\"))\n",
        "            model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "            self.model=model\n",
        "        print(model.summary())\n",
        "\n",
        "\n",
        "    def train(self, feature, target, num_epochs=50):\n",
        "        checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "        # reduce_LR = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "        # logdir='logsnextword1'\n",
        "        # tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
        "        self.model.compile(optimizer=Adam(lr=0.001), loss='SparseCategoricalCrossentropy', metrics=['acc'])\n",
        "        self.model.fit(feature, target, epochs=num_epochs, batch_size=64, validation_split=0.1, callbacks=[checkpoint])#, reduce_LR, tensorboard_Visualization])\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCwDvVrSM-AV"
      },
      "source": [
        "X1,y1, vocab_size1 = preprocess_sentence(base_path='', file_name='train_orig.txt',news_id='1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4CvkaL6M_JC",
        "outputId": "22b759be-8758-4b5c-a0cb-19d6152d7117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "classifier = nwp_model(vocab_size=vocab_size1,embedding_dim=40)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 30, 40)            923120    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 30, 40)            12960     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 30, 40)            1640      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30, 23078)         946198    \n",
            "=================================================================\n",
            "Total params: 1,883,918\n",
            "Trainable params: 1,883,918\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRyX-bJ0ZA61",
        "outputId": "d68ab0c6-fa14-4b93-c78d-8b76c3863b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classifier.train(X1,y1, num_epochs=20)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 3.4425 - acc: 0.0662\n",
            "Epoch 00001: loss improved from inf to 3.44265, saving model to nextword1.h5\n",
            "274/274 [==============================] - 31s 112ms/step - loss: 3.4427 - acc: 0.0662 - val_loss: 3.1286 - val_acc: 0.0727\n",
            "Epoch 2/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 3.0720 - acc: 0.0729\n",
            "Epoch 00002: loss improved from 3.44265 to 3.07212, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 108ms/step - loss: 3.0721 - acc: 0.0729 - val_loss: 3.0939 - val_acc: 0.0758\n",
            "Epoch 3/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 3.0120 - acc: 0.0904\n",
            "Epoch 00003: loss improved from 3.07212 to 3.01167, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 108ms/step - loss: 3.0117 - acc: 0.0904 - val_loss: 3.0267 - val_acc: 0.1028\n",
            "Epoch 4/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.9053 - acc: 0.1114\n",
            "Epoch 00004: loss improved from 3.01167 to 2.90537, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.9054 - acc: 0.1114 - val_loss: 2.9428 - val_acc: 0.1201\n",
            "Epoch 5/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.8119 - acc: 0.1255\n",
            "Epoch 00005: loss improved from 2.90537 to 2.81168, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 108ms/step - loss: 2.8117 - acc: 0.1255 - val_loss: 2.8949 - val_acc: 0.1304\n",
            "Epoch 6/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.7459 - acc: 0.1398\n",
            "Epoch 00006: loss improved from 2.81168 to 2.74575, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.7457 - acc: 0.1398 - val_loss: 2.8680 - val_acc: 0.1406\n",
            "Epoch 7/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.6968 - acc: 0.1480\n",
            "Epoch 00007: loss improved from 2.74575 to 2.69689, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.6969 - acc: 0.1480 - val_loss: 2.8501 - val_acc: 0.1448\n",
            "Epoch 8/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.6564 - acc: 0.1553\n",
            "Epoch 00008: loss improved from 2.69689 to 2.65640, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 108ms/step - loss: 2.6564 - acc: 0.1553 - val_loss: 2.8337 - val_acc: 0.1502\n",
            "Epoch 9/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.6207 - acc: 0.1609\n",
            "Epoch 00009: loss improved from 2.65640 to 2.62030, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.6203 - acc: 0.1610 - val_loss: 2.8177 - val_acc: 0.1542\n",
            "Epoch 10/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.5873 - acc: 0.1656\n",
            "Epoch 00010: loss improved from 2.62030 to 2.58746, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.5875 - acc: 0.1656 - val_loss: 2.8051 - val_acc: 0.1586\n",
            "Epoch 11/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.5577 - acc: 0.1701\n",
            "Epoch 00011: loss improved from 2.58746 to 2.55753, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.5575 - acc: 0.1701 - val_loss: 2.7951 - val_acc: 0.1605\n",
            "Epoch 12/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.5293 - acc: 0.1744\n",
            "Epoch 00012: loss improved from 2.55753 to 2.52966, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.5297 - acc: 0.1744 - val_loss: 2.7865 - val_acc: 0.1625\n",
            "Epoch 13/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.5026 - acc: 0.1795\n",
            "Epoch 00013: loss improved from 2.52966 to 2.50252, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.5025 - acc: 0.1795 - val_loss: 2.7809 - val_acc: 0.1645\n",
            "Epoch 14/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.4760 - acc: 0.1847\n",
            "Epoch 00014: loss improved from 2.50252 to 2.47613, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 108ms/step - loss: 2.4761 - acc: 0.1847 - val_loss: 2.7747 - val_acc: 0.1679\n",
            "Epoch 15/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.4506 - acc: 0.1897\n",
            "Epoch 00015: loss improved from 2.47613 to 2.45056, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.4506 - acc: 0.1898 - val_loss: 2.7709 - val_acc: 0.1694\n",
            "Epoch 16/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.4257 - acc: 0.1951\n",
            "Epoch 00016: loss improved from 2.45056 to 2.42568, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.4257 - acc: 0.1951 - val_loss: 2.7682 - val_acc: 0.1697\n",
            "Epoch 17/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.4015 - acc: 0.1992\n",
            "Epoch 00017: loss improved from 2.42568 to 2.40169, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.4017 - acc: 0.1992 - val_loss: 2.7676 - val_acc: 0.1703\n",
            "Epoch 18/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.3784 - acc: 0.2032\n",
            "Epoch 00018: loss improved from 2.40169 to 2.37816, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.3782 - acc: 0.2032 - val_loss: 2.7672 - val_acc: 0.1707\n",
            "Epoch 19/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.3546 - acc: 0.2068\n",
            "Epoch 00019: loss improved from 2.37816 to 2.35449, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.3545 - acc: 0.2068 - val_loss: 2.7697 - val_acc: 0.1710\n",
            "Epoch 20/20\n",
            "273/274 [============================>.] - ETA: 0s - loss: 2.3319 - acc: 0.2097\n",
            "Epoch 00020: loss improved from 2.35449 to 2.33211, saving model to nextword1.h5\n",
            "274/274 [==============================] - 30s 109ms/step - loss: 2.3321 - acc: 0.2097 - val_loss: 2.7718 - val_acc: 0.1720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa1x-21vqHVB"
      },
      "source": [
        "### Plot The Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouU2ao7DqHVD"
      },
      "source": [
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWyx7TDRwFea"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"./logsnextword1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEEjeyhdu595"
      },
      "source": [
        "# Importing the Libraries\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "model = load_model('nextword1.h5')\n",
        "tokenizer = pickle.load(open('tokenizer_train_orig.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text, num_words_to_complete):\n",
        "    \"\"\"\n",
        "        In this function we are using the tokenizer and models trained\n",
        "        and we are creating the sequence of the text entered and then\n",
        "        using our model to predict and return the the predicted word.\n",
        "    \n",
        "    \"\"\"\n",
        "    res = \"\"\n",
        "    text_len = len(text)\n",
        "    for _ in range(num_words_to_complete):\n",
        "        sequence = tokenizer.texts_to_sequences([text])\n",
        "        padded_sequence = pad_sequences(sequence, padding=\"post\", maxlen=30)\n",
        "        output = model.predict(padded_sequence)\n",
        "        preds = np.argmax(output, axis=-1)\n",
        "        predicted_word = tokenizer.sequences_to_texts(preds)[0].split()[text_len-1]\n",
        "        print(predicted_word)\n",
        "        res += predicted_word + ' '\n",
        "        text += predicted_word\n",
        "        text_len+=1\n",
        "\n",
        "    print(res)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzRssTdpu711",
        "outputId": "90ae6e43-8c2d-430a-b1ab-721fe52e6cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "\"\"\"\n",
        "    We are testing our model and we will run the model\n",
        "    until the user decides to stop the script.\n",
        "    While the script is running we try and check if \n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\n",
        "\n",
        "\"\"\"\n",
        "num_words_to_complete = int(input(\"Enter number of words to complete:\"))\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter beginning of sentence: \") \n",
        "    if text == \"x\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        text = text.split(\" \")\n",
        "        Predict_Next_Words(model, tokenizer, text, num_words_to_complete)\n",
        "        "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of words to complete:5\n",
            "Enter beginning of sentence: donald\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2ad2e646a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "trump\n",
            "said\n",
            "and\n",
            "and\n",
            "oov_word\n",
            "trump said and and oov_word \n",
            "Enter beginning of sentence: trump campaign\n",
            "and\n",
            "new\n",
            "oov_word\n",
            "oov_word\n",
            "oov_word\n",
            "and new oov_word oov_word oov_word \n",
            "Enter beginning of sentence: x\n",
            "Ending The Program.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ9ou7BjPf4o"
      },
      "source": [
        "X7,y7, vocab_size7 = preprocess_sentence(base_path='', file_name='train_orig.txt',news_id='7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdG-ABJ-PkHb"
      },
      "source": [
        "classifier = nwp_model(embedding_dim=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1843k_5fPj-k"
      },
      "source": [
        "classifier.train(X7,y7, num_epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdtcTYjKSRDO"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}